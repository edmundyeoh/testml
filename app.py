# -*- coding: utf-8 -*-
"""testml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LLcujOLNVI063OSBE02bd2a5yYgCwtSJ
"""

pip install transformers

import pickle

pip install PyPDF2

# Import necessary libraries and modules
import torch
from transformers import BertModel, BertTokenizer

# Load a pre-trained BERT model and tokenizer
model_name = 'bert-base-uncased'
model = BertModel.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

def classify_text_with_bert(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    # Access the model's classification head logits
    logits = outputs.last_hidden_state

    # Apply softmax to the logits to get probability scores
    probabilities = torch.softmax(logits, dim=1)

    # Get the predicted sentiment class index for each item in the batch
    predicted_classes = torch.argmax(probabilities, dim=1)

    # Convert the tensor of predicted classes to a Python list
    predicted_classes = predicted_classes.tolist()

    return predicted_classes

def classify_text_with_bert(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)

    # Access the model's classification head logits
    logits = outputs.last_hidden_state

    # Apply softmax to the logits to get probability scores
    probabilities = torch.softmax(logits, dim=1)

    # Get the predicted sentiment class index for each item in the batch
    predicted_classes = torch.argmax(probabilities, dim=1)

    # Convert the tensor of predicted classes to a Python list
    predicted_classes = predicted_classes.tolist()

    return predicted_classes

# Define a function to perform ABSA on text
def perform_absa(text, aspect):
    # Implement aspect extraction logic if needed
    # This can involve using NER or other techniques to identify the aspect in the text
    aspect_text = ["experience", "skills", "education", "achievements", "soft skills", "Projects "]

    # Perform aspect-based sentiment analysis and return the sentiment score
    sentiment_score = results[0]['label']


    return sentiment_score

# Import the necessary libraries and modules
from transformers import pipeline
def perform_absa(resume_text):
    # Define the sentiment analysis pipeline
    sentiment_pipeline = pipeline("sentiment-analysis")

    # Analyze the sentiment of the entire resume
    results = sentiment_pipeline(resume_text)

    # Extract the sentiment score (label) from the result
    sentiment_score = results[0]['label']

    return sentiment_score

# Import the necessary libraries and modules
from transformers import pipeline

# Define the sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

import os
import PyPDF2
from transformers import pipeline

# Define a function to perform ABSA on text
def perform_absa(text):
    # Define the sentiment analysis pipeline with a specific model
    sentiment_pipeline = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

    # Split the text into smaller chunks (you can adjust the chunk size as needed)
    chunk_size = 500  # Number of characters per chunk
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    sentiment_scores = []

    for chunk in chunks:
        # Analyze the sentiment of each chunk
        results = sentiment_pipeline(chunk)

        # Extract the sentiment score from the result
        sentiment_score = results[0]['score']

        sentiment_scores.append(sentiment_score)

    # Calculate the average sentiment score for all chunks
    avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)

    return avg_sentiment_score

# Create a list to store the text content and sentiment scores of each resume
resumes_data = []

# Define the directory where the resume PDF files are located
resume_dir = "/content"

# Iterate over files in the directory
for filename in os.listdir(resume_dir):
    if filename.endswith('.pdf'):
        # Form the complete path to the PDF file
        pdf_file = os.path.join(resume_dir, filename)

        # Open and read each PDF file
        with open(pdf_file, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page_num, page in enumerate(pdf_reader.pages):
                text += page.extract_text()

        # Perform ABSA on the resume text
        sentiment_score = perform_absa(text)

        # Append the resume data (score) to the list
        resumes_data.append((filename, sentiment_score))

# Sort resumes by sentiment score in descending order
sorted_resumes = sorted(resumes_data, key=lambda x: x[1], reverse=True)

# Print the ranked list of resumes (without content, just filenames and sentiment scores)
for rank, (filename, score) in enumerate(sorted_resumes, start=1):
    print(f"Rank {rank}: Resume: {filename}, Sentiment Score: {score}")

# Import the necessary libraries and modules
from transformers import pipeline

# Define a function to perform ABSA on text
def perform_absa(text):
    # Define the sentiment analysis pipeline
    sentiment_pipeline = pipeline("sentiment-analysis")

    # Analyze the sentiment of the entire text
    result = sentiment_pipeline(text)

    # Extract the sentiment score (label) from the result
    sentiment_score = result[0]['label']

    return sentiment_score

# Define a function to perform ABSA on multiple aspects of a resume
def analyze_resume_aspects(resume_text):
    aspects = {
        "Experience": "experience",
        "Skills": "skills",
        "Education": "education",
        "Achievements": "achievements",
        "Soft Skills": "soft skills",
        "Projects": "projects",
        "Objective": "objective or summary",
        "Certifications": "certifications",
        "Languages": "languages",
        "Hobbies": "hobbies",

    }

    aspect_sentiments = {}

    for aspect_name, aspect_key in aspects.items():
        aspect_text = extract_aspect_from_resume(resume_text, aspect_key)
        sentiment = perform_absa(aspect_text)
        aspect_sentiments[aspect_name] = sentiment

    return aspect_sentiments

import re

def extract_aspect_from_resume(resume_text, aspect_key):
    # Use regular expressions or other methods to extract the specific aspect from the resume text
    # Here, we use a simple example using a keyword (aspect_key)
    aspect_matches = re.findall(rf'{aspect_key}:(.*?)(?:\w+:|$)', resume_text, re.DOTALL)

    # Join the extracted text into a single string
    extracted_text = '\n'.join(aspect_matches)

    return extracted_text


# Use the extraction function to get the "experience" aspect text
def extract_experience_from_resume(aspect_text):
    aspect_text = extract_experience_from_resume(text)

# Filter out resumes with None overall scores
filtered_resumes_data = [(filename, score) for filename, score in resumes_data if score is not None]

# Sort resumes by overall score in descending order (from highest to lowest)
sorted_resumes = sorted(filtered_resumes_data, key=lambda x: x[1], reverse=True)

# Print the ranked list of resumes (without content, just filenames and scores)
for idx, (filename, score) in enumerate(sorted_resumes, start=1):
    print(f"Rank {idx}: Filename: {filename}, Overall Score: {score}")

filename= 'testml'
pickle.dump(model, open(filename,'wb'))

